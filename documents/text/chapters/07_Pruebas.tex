\chapter{Pruebas}

En el capítulo anterior se describía como se había realizado toda la implementación del proyecto, por lo que ya solo queda probar que todo funciona correctamente.

\section{Pruebas unitarias}

Para ejecutar las pruebas unitarias vamos a ejecutar con {\tt Mocha} a través de {\tt Istanbul}, realizando las pruebas unitarias y generando el resultado de la prueba de cobertura en un misma ejecución. Cuando ejecutemos {\tt npm test} comenzarán a comprobarse una por una todas las comprobaciones en el archivo {\tt tests.js}; las llamadas al método {\tt describe} son las líneas que aparecen como título en blanco mientras que las comprobaciones que se hacían con el método {\tt it} son las líneas en gris, que aparecen con un \textit{check} verde si el valor esperado es el correcto ó, en caso de error, con un número rojo que indique el error de forma enumerada.

\bigskip
Una vez que todas las comprobaciones hayan sido realizadas nos aparecerá un resumen con los resultados de los test pasados, indicando cuantos se han ejecutado correctamente (\textbf{passing}) y cuantos han producido un error (\textbf{failing}); si alguna de las tareas asíncronas no devolviera el mensaje {\tt done} estas quedarían como comprobaciones pendientes (\textbf{pending}), pero esto es algo que no se produce en nuestro caso.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/tests_unitarios_01.png}
		\caption{Ejecución de los test unitarios con Mocha (1/2)}
		\label{fig:tests_unitarios_01}
	\end{center}
\end{figure}

\newpage
En la siguiente imagen vemos que todas los tests se han pasado correctamente (\textbf{148 passing}) en un tiempo de \textbf{3 segundos}. Además, como hemos dicho que {\tt Mocha} es ejecutado a través de {\tt Istanbul}, también obtenemos el mensaje de que el test de cobertura se ha generado dentro de la carpeta {\tt coverage} y un resumen de la cobertura del código que realizan nuestros test.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/tests_unitarios_02.png}
		\caption{Ejecución de los test unitarios con Mocha (2/2)}
		\label{fig:tests_unitarios_02}
	\end{center}
\end{figure}

\newpage
Si alguno de los test fallara (por ejemplo, uno de los archivos de origen de datos no existe) en el resumen además del número de test fallados, se mostrará una descripción de los mismos. Es el caso de la siguiente imagen, el archivo {\tt personal.json} no existe por lo que entre los 11 errores que se han producido vemos que en la comprobación de la existencia del archivo se produce un aserción debido a que el resultado esperado por {\tt Mocha} es \textbf{true} y sin embargo recibe \textbf{false} en su lugar}, relacionado con esto nos encontramos el siguiente error y es que como no se ha podido cargar el archivo no se han podido comprobar las propiedades que deberían tener ese archivo (la propiedad que espera \textbf{should} es nula).

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/tests_unitarios_03.png}
		\caption{Error en tests unitarios}
		\label{fig:tests_unitarios_03}
	\end{center}
\end{figure}

\section{Prueba de cobertura}

Como veíamos al finalizar los test unitarios, el resultado del test de cobertura se almacena en la carpeta {\tt coverage}, dentro de dicha carpeta encontraremos un archivo \textit{HTML} con el mismo resumen que aparecía al finalizar la ejecución de los tests unitarios, pero en este caso permitiéndonos acceder a una información más completa. 

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/test_cobertura_01.png}
		\caption{Resumen de resultados de test de cobertura}
		\label{fig:test_cobertura_01}
	\end{center}
\end{figure}

Por ejemplo, vamos a comprobar más detalladamente la cobertura en el código de la aplicación principal de la plataforma, por lo que seleccionamos \textbf{ugr-transparente-servidor} y después el archivo de nuestra aplicación {\tt app.js}. En el resumen todo el código de esta parte de la aplicación aparecía con una cobertura del \textbf{100\%} por eso todas las líneas del código aparecen en verde.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/test_cobertura_02.png}
		\caption{Resultado de cobertura de la aplicación principal}
		\label{fig:test_cobertura_02}
	\end{center}
\end{figure}

En el caso de que alguna sentencia no estuviera cubierta por los test unitarios, esta aparecería en rojo; esto es lo que sucede por ejemplo en uno de los módulos desarrollado: cuando se captura una excepción espera que sea lanzada a un nivel superior sin embargo solo se muestra un mensaje por pantalla, {\tt Istanbul} aprecia esto como un fallo de cobertura y por eso destaca esa línea en rojo.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/test_cobertura_03.png}
		\caption{Resultado de cobertura de un módulo desarrollado}
		\label{fig:test_cobertura_03}
	\end{center}
\end{figure}

\newpage
\section{Integración continua}

Tenemos los test unitarios hecho y hemos comprobado que funcionan, por lo que teniendo la configuración de {\tt Travis CI} realizada, cada vez que se haga un \textit{push} al repositorio se generará un \textit{build} en sus servidores para comprobar que los nuevos cambios introducidos no producen errores en la aplicación. Esta comprobación se hará en cada una de las versiones de nuestro lenguaje que hayamos definido, obteniendo un resumen al final de su ejecución que nos indicará si todo se ha ejecutado correctamente; dicho resultado es accesible a través de su página web (\url{https://travis-ci.org/oslugr/ugr-transparente-servidor}). En la imagen siguiente tenemos el resultado de un \textit{build} en el que no se ha producido ningún error, por lo que todos pruebas de las diferentes versiones están marcadas con un \textit{check} verde.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/integracion_continua_01.png}
		\caption{Build en Travis CI generado con éxito}
		\label{fig:integracion_continua_01}
	\end{center}
\end{figure}

Si seleccionamos en concreto alguna de las entradas, obtendremos la salida del despliegue de la aplicación y la posterior ejecución de los test.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/integracion_continua_02.png}
		\caption{Salida del build en Travis CI (despliegue)}
		\label{fig:integracion_continua_02}
	\end{center}
\end{figure}

La salida de la ejecución de los tests es exactamente igual a la salida que hemos visto que se produce se ejecutamos los tests de forma local llamando a {\tt npm test} (que es lo hace {\tt Travis} igualmente).

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/integracion_continua_03.png}
		\caption{Salida del build en Travis CI (resultados tests)}
		\label{fig:integracion_continua_03}
	\end{center}
\end{figure}

\section{Despliegue automático}

También vamos a comprobar que el despliegue automático funciona correctamente. Simplemente ejecutamos el script {\tt deploy} pasándole como parámetro el usuario con el que nos conectaremos al servidor. En la imagen siguiente vemos el resultado de la ejecución, aunque no en este momento no había nuevos cambios que aplicar al servidor, si podemos ver como hace la copia de seguridad inicial y ejecuta el resto de órdenes necesarias para que la aplicación del portal quede en funcionamiento.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/deploy.png}
		\caption{Ejecución de despliegue automático}
		\label{fig:deploy}
	\end{center}
\end{figure}

\newpage
\section{Provisionamiento}

Como el sitio de {\tt UGR Transparente} está funcionando sin problemas en este momento, para probar el provisionamiento crearemos una máquina virtual con {\tt Vagrant} con el mismo sistema operativo que se encuentra en el servidor de la plataforma ({\tt Ubuntu 14.04}) para realizarle el provisionamiento con el {\tt playbook} escrito para {\tt Ansible}.

\bigskip
Las órdenes a introducir para crear la máquina virtual son las que aparecen en el siguiente fragmento de código.

\begin{lstlisting}[language=bash,captionpos=b,caption={Órdenes para crear máquina virtual Vagrant},label={lst:crear_vagrant}]
vagrant box add ubuntu/trusty64
vagrant init ubuntu/trusty64
vagrant up
vagrant ssh
\end{lstlisting}

\newpage
También tenemos que crear un archivo {\tt Vagrantfile} en el que es importante establecer la dirección de acceso a la máquina y como provisionador {\tt Ansible}, indicando también el \textit{playbook} que se va a utilizar.

\begin{lstlisting}[language=Ruby,captionpos=b,caption={Archivo Vagrantfile},label={lst:edit_vgfile}]
# -*- mode: ruby -*-
# vi: set ft=ruby :

VAGRANTFILE_API_VERSION = "2"

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
	config.vm.box = "ubuntu/trusty64"
	config.vm.network :private_network, ip:"192.168.2.50"

	config.vm.provision "ansible" do |ansible|
		ansible.playbook = "transparente.yml"
	end
end
\end{lstlisting}

Lo siguiente será cambiar en el archivo {\tt ansible\_hosts} la dirección del portal por la dirección de nuestra máquina virtual (comando {\tt sed}), declarar como variable de entorno archivo {\tt ansible\_hosts} (comando {\tt export}), y finalmente con {\tt Vagrant}, recargar la configuración del {\tt Vagrantfile} ({\tt vagrant reload}) y ordenar el provisionamiento de la máquina virtual ({\tt vagrant provision}). En el siguiente fragmento de código se listan todos estos comandos y en la imagen que le sigue se puede comprobar la ejecución, obteniendo como resultado un provisionamiento completamente exitoso.

\begin{lstlisting}[language=bash,captionpos=b,caption={Órdenes para provisionar máquina Vagrant},label={lst:aprov_vagrant}]
sed "s/transparente.ugr.es/192.168.2.50/" -i ansible_hosts
export ANSIBLE_HOSTS=ansible_hosts
vagrant reload
vagrant provision
\end{lstlisting}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/provision.png}
		\caption{Prueba de provisionamiento con Ansible}
		\label{fig:provision}
	\end{center}
\end{figure}

\newpage
\section{Prueba de carga}

Una vez que ya hemos realizado las pruebas de software que hemos considerado oportunas, también tenemos que pasar pruebas a la infraestructura como tal para analizar su rendimiento. En este tipo de pruebas entra en juego tanto el aspecto software como hardware con el objetivo de obtener una medición del rendimiento de nuestra aplicación.

\newpage
\subsection{Métricas y parámetros que afectan al rendimiento}

Para comparar las prestaciones de la aplicación debemos tener en cuenta los siguientes criterios:

\begin{itemize}
	\item El objetivo de esta prueba es medir las prestaciones del servidor generado por {\tt Express} para dar servicio a la aplicación del portal de transparencia bajo unas condiciones que nos aporte un análisis neutro del rendimiento del mismo.
	\item La herramienta que se usará para realizar estás mediciones es {\tt ApacheBench}.
	\item Los parámetros que se considerarán los parámetros usados en la herramienta: el número de peticiones que se realizan al servidor y el nivel de concurrencia con el que se realizan las peticiones.
	\item Los valores de estos parámetros irán modificándose para tener unos resultados más completos ante las diferentes cargas de trabajo que soportará el servidor.
\end{itemize}

El hardware y el software del sistema serán los siguientes:

\begin{itemize}
	\item \textbf{Hardware}:
	\begin{itemize}
		\item Procesador: Intel Pentium Dual CPU E2180 @ 2.00GHz
		\item Placa base: MSI MS-7255
		\item Chipset: VIA P4M900
		\item Memoria: 3GB (2+1 DIMM DDR2)
		\item Disco: Maxtor 6Y160P0 160GB 7200RPM
		\item Tarjeta gráfica: ATI Radeon X300SE 325MHz 128MB
		\item Red: Realtek RTL8100C 100Mbps
	\end{itemize}
	\item \textbf{Software}:
	\begin{itemize}
		\item Sistema operativo: Ubuntu 14.04.1 LTS i686 GNU/Linux
		\item Kernel: Linux 3.13.0-35-generic
		\item Sistema de archivos: ext4
	\end{itemize}
\end{itemize}

\newpage
\subsection{Técnicas de evaluación, carga de trabajo y diseño de experimentos}

Para evaluar el rendimiento de la aplicación vamos a realizar benchmark hacia la aplicación en ejecución para poder realizar una evaluación sobre su comportamiento bajo diferentes cargas de trabajo. El programa con el que vamos hacer las pruebas es el ya mencionado {\tt ApacheBench}, que nos permitirá realizar de forma sencilla pruebas de rendimiento a cualquier servidor, sea cual sea el lenguaje en el que esté realizado. 

\bigskip
Según la información de registros de acceso del servidor en los últimos \textbf{6 meses} el número de peticiones de páginas del portal ha sido de \textbf{2.388 peticiones}, lo que sería aproximadamente \textbf{13 peticiones/día}. Para realizar los diferentes tests se realizará un \textbf{número de conexiones} variables al servidor (\textbf{30, 50 y 100}) con diferentes \textbf{niveles de concurrencia} en función del total de conexiones (\textbf{25\%, 50\% y 75\%}). Los números de conexiones para las prueba han sido elegidos para evaluar como se comportaría el servidor ante un gran aumento de actividad en el mismo en línea con el nivel de conexiones que se producen en la actualidad.

\bigskip
De entre las peticiones al portal, la página que ha recibido un mayor número de peticiones es la página de \textbf{Personal}, por lo que el experimento a realizar consistirá en realizar peticiones de la página de \textbf{Personal} a la aplicación. Este experimente nos permitirá comprobar como se comportará la aplicación en diferentes situaciones cono diferentes niveles de carga y la repercusión en su rendimiento ante estás diferentes pruebas, viendo por ejemplo si fuera necesario buscar una forma de balancear la carga del servidor. 

\bigskip
En total el experimento constará de 9 pruebas, y a su vez cada una de estas pruebas se repetirá 10 veces consecutivas para así asegurarnos que los resultados son legítimos y no producto de sucesos fortuitos. Los resultados mostrados serán la media de esas 10 ejecuciones y su desviación estándar, que nos permitirá saber como de válidos podemos considerar los valores medios obtenidos. Estos valores promedios se introducirán en un gráfico para comparar los cambios que se producen con diferentes números de conexiones y mismos porcentaje de concurrencia. Las variables respuestas a tener en cuenta para el estudio serán:

\begin{itemize}
	\item Tiempo de ejecución.
	\item Solicitudes por segundo.
	\item Tiempo por solicitud concurrente.
	\item Velocidad de transferencia.
\end{itemize}

\bigskip
El único factor a considerar para el experimento será, como hemos
dicho, las respuestas del servidor de la aplicación desarrollada del
portal de transparencia {\tt UGR Transparente}. Cuando se disponga de
los resultados de todas las pruebas, se procederá a analizar e
interpretar los resultados. % ¿Y cuando se va a disponer? - JJ

\subsection{Presentación de los resultados}

Enumeración de las pruebas a realizar en el experimento:

\begin{itemize}
	\item \textbf{Prueba 1}: 30 solicitudes, concurrencia del 25\% (8).
	\item \textbf{Prueba 2}: 30 solicitudes, concurrencia del 50\% (15).
	\item \textbf{Prueba 3}: 30 solicitudes, concurrencia del 75\% (23).
	\item \textbf{Prueba 4}: 50 solicitudes, concurrencia del 25\% (13).
	\item \textbf{Prueba 5}: 50 solicitudes, concurrencia del 50\% (25).
	\item \textbf{Prueba 6}: 50 solicitudes, concurrencia del 75\% (38).
	\item \textbf{Prueba 7}: 100 solicitudes, concurrencia del 25\% (25).
	\item \textbf{Prueba 8}: 100 solicitudes, concurrencia del 50\% (50).
	\item \textbf{Prueba 9}: 100 solicitudes, concurrencia del 75\% (75).
\end{itemize}

\subsubsection{Tiempo de ejecución}
\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{4}{|c|}{{\bf Tiempo de ejecución}}                                                           \\ \hline
			{\bf N.º conexiones} & {\bf Concurrencia (\%)} & {\bf Tiempo (s)} & {\bf Desviación} \\ \hline
			{\it 30}                   & {\it 25}                  & 8,823              & 0,199                     \\ \hline
			{\it 30}                   & {\it 50}                  & 7,552              & 0,214                     \\ \hline
			{\it 30}                   & {\it 75}                  & 6,827              & 0,970                     \\ \hline
			{\it 50}                   & {\it 25}                  & 10,215             & 0,248                     \\ \hline
			{\it 50}                   & {\it 50}                  & 12,506             & 0,299                     \\ \hline
			{\it 50}                   & {\it 75}                  & 13,959             & 0,217                     \\ \hline
			{\it 100}                  & {\it 25}                  & 26,966             & 1,781                     \\ \hline
			{\it 100}                  & {\it 50}                  & 22,850             & 3,845                     \\ \hline
			{\it 100}                  & {\it 75}                  & 24,473             & 3,845                     \\ \hline
		\end{tabular}
		\caption{Resultados de tiempo de ejecución}
		\label{table:rte}
	\end{center}
\end{table}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{../graphics/gra_te.png}
		\caption{Gráfico de tiempo de ejecución}
		\label{fig:gra_te}
	\end{center}
\end{figure}

\newpage
\subsubsection{Solicitudes por segundo}
\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{4}{|c|}{{\bf Solicitudes por segundo}}                                                   \\ \hline
			{\bf N.º de conexiones} & {\bf Concurrencia (\%)} & {\bf Solicitudes/s} & {\bf Desviación} \\ \hline
			{\it 30}                & {\it 25}                & 3,402                          & 0,078            \\ \hline
			{\it 30}                & {\it 50}                & 3,976                          & 0,113            \\ \hline
			{\it 30}                & {\it 75}                & 4,470                          & 0,531            \\ \hline
			{\it 50}                & {\it 25}                & 4,898                          & 0,118            \\ \hline
			{\it 50}                & {\it 50}                & 4,000                          & 0,095            \\ \hline
			{\it 50}                & {\it 75}                & 3,583                          & 0,056            \\ \hline
			{\it 100}               & {\it 25}                & 3,728                          & 0,292            \\ \hline
			{\it 100}               & {\it 50}                & 4,496                          & 0,708            \\ \hline
			{\it 100}               & {\it 75}                & 4,197                          & 0,703            \\ \hline
		\end{tabular}
		\caption{Resultados de solicitudes por segundo}
		\label{table:rss}
	\end{center}
\end{table}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{../graphics/gra_sps.png}
		\caption{Gráfico de solicitudes por segundo}
		\label{fig:gra_sps}
	\end{center}
\end{figure}

\newpage
\subsubsection{Tiempo por solicitud concurrente}
\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{4}{|c|}{{\bf Tiempo por solicitud concurrente}}                               \\ \hline
			{\bf N.º de conexiones} & {\bf Concurrencia (\%)} & {\bf Tiempo (ms)} & {\bf Desviación} \\ \hline
			{\it 30}                & {\it 25}                & 294,087             & 6,645            \\ \hline
			{\it 30}                & {\it 50}                & 251,733             & 7,129            \\ \hline
			{\it 30}                & {\it 75}                & 227,563             & 32,347           \\ \hline
			{\it 50}                & {\it 25}                & 204,296             & 4,959            \\ \hline
			{\it 50}                & {\it 50}                & 250,128             & 5,990            \\ \hline
			{\it 50}                & {\it 75}                & 279,178             & 4,346            \\ \hline
			{\it 100}               & {\it 25}                & 269,658             & 17,811           \\ \hline
			{\it 100}               & {\it 50}                & 228,497             & 38,451           \\ \hline
			{\it 100}               & {\it 75}                & 244,729             & 38,449           \\ \hline
		\end{tabular}
		\caption{Resultados de tiempo por solicitud concurrente}
		\label{table:rtsc}
	\end{center}
\end{table}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{../graphics/gra_tsc.png}
		\caption{Gráfico de tiempo por solicitud concurrente}
		\label{fig:gra_tsc}
	\end{center}
\end{figure}

\newpage
\subsubsection{Velocidad de transparencia}
\begin{table}[!ht]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\multicolumn{4}{|c|}{{\bf Velocidad de transferencia}}                            \\ \hline
			{\bf N.º de conexiones} & {\bf Concurrencia (\%)} & {\bf Velocidad (KB/s)} & {\bf Desviación} \\ \hline
			{\it 30}                & {\it 25}                & 747,735    & 17,116           \\ \hline
			{\it 30}                & {\it 50}                & 873,789    & 24,798           \\ \hline
			{\it 30}                & {\it 75}                & 982,389    & 116,778          \\ \hline
			{\it 50}                & {\it 25}                & 645,866    & 15,498           \\ \hline
			{\it 50}                & {\it 50}                & 527,515    & 12,561           \\ \hline
			{\it 50}                & {\it 75}                & 472,469    & 7,339            \\ \hline
			{\it 100}               & {\it 25}                & 245,783    & 19,227           \\ \hline
			{\it 100}               & {\it 50}                & 296,416    & 46,684           \\ \hline
			{\it 100}               & {\it 75}                & 276,707    & 46,384           \\ \hline
		\end{tabular}
		\caption{Resultados de velocidad de transferencia}
		\label{table:rvt}
	\end{center}
\end{table}

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=0.8\textwidth]{../graphics/gra_vt.png}
		\caption{Gráfico de velocidad de transferencia}
		\label{fig:gra_vt}
	\end{center}
\end{figure}

\newpage
\subsection{Análisis e interpretación de los resultados}

\subsubsection{Tiempo de ejecución}
\begin{itemize}
	\item Para un número de 30 conexiones (poco más de la media de conexiones diarias al portal) la concurrencia no afecta al tiempo de respuesta de la aplicación.
	\item Cuando el número de conexiones aumenta hasta 50, vemos como en este el nivel de concurrencia sí empieza a afectar al tiempo de respuesta de la aplicación de forma negativa.
	\item En la última prueba con un número de conexiones bastante más elevado, vemos en los tiempos de ejecución que el comportamiento es más aleatorio. Si nos fijamos en las desviaciones estás han crecido bastante en comparación con las pruebas anteriores, por lo que aparentemente la aplicación empieza a soportar cierta sobrecarga.
\end{itemize}

\subsubsection{Solicitudes por segundo}
\begin{itemize}
	\item En relación directa con los resultados descritos en el apartado anterior vemos que para 30 conexiones, al igual que el tiempo de respuesta bajaba según el nivel de concurrencia, ahora el número de solicitudes respondidas por segundo aumentan; el comportamiento que cabría esperar.
	\item En el caso de 50 conexiones, de igual forma, cuando el nivel de concurrencia sube, el número de solicitudes respondidas baja.
	\item Para la prueba de 100 conexiones volvemos a tener una situación similar a la anterior, donde aunque en esta caso las desviaciones no son tan altas, no se puede concretar una tendencia en el rendimiento de la aplicación.
\end{itemize}

\subsubsection{Tiempo por solicitud concurrente}
\begin{itemize}
	\item Se produce la misma situación, para 30 conexiones el tiempo necesario para resolver una solicitud recurrente va disminuyendo según aumenta la concurrencia en las conexiones. A destacar que en esta prueba el valor de la desviación crece en gran cantidad, por lo que aunque se puede encontrar una tendencia general de mejor en el rendimiento los tiempos para resolver las solicitudes no son tan homogéneos.
	\item En el caso de las 50 conexiones también se produce la misma situación a las pruebas anteriores: según aumenta la concurrencia aumenta el tiempo necesario para responder una solicitud concurrente, lo que indica que el rendimiento ha bajado.
	\item De igual forma, para 100 conexiones volvemos a encontrar sin un patrón claro de comportamiento, pero si podemos destacar que las desviaciones son enormes.
\end{itemize}

\subsubsection{Velocidad de transferencia}
\begin{itemize}
	\item Mismo patrón para la prueba con 30 conexiones. Habíamos visto que según aumenta la concurrencia, disminuye el tiempo de ejecución y aumentan el número de solicitudes por segundo respondidas o lo que es lo mismo, aumenta el rendimiento; así que como podríamos intuir y con estos datos comprobar, también aumenta la cantidad de información transferida. Nos encontramos en algunos casos con desviaciones enormes, pero eso podemos atribuirlo a la propia naturaleza inestable de las conexiones de red.
	\item Como se ha venido produciendo, en el caso de la prueba con 50 conexiones el rendimiento ha ido bajando según ha ido aumentando el porcentaje de concurrencia; esto también se produce en este caso: según aumentaba la concurrencia, disminuía la velocidad de transferencia.
	\item Por último, en la prueba con 100 conexiones esta es la prueba que ha tenido resultados más igualados (y además muy bajos en comparación) entre los diferentes niveles de concurrencia, lo que nos vuelve a hacer suponer que el servidor está sobrecargado.
\end{itemize}

\subsection{Conclusiones sobre los resultados}

Una vez analizados los resultados de las pruebas realizadas la conclusión a la que podríamos llegar
es que la aplicación puede aguantar perfectamente la carga de trabajo que tiene en la actualidad (alrededor de 13 peticiones al día), y que tampoco no habría ningún problema en que esta se duplicara como hemos visto en los resultados de las pruebas de 30 conexiones. El problema aparecería si este volumen de visitas siguiera creciendo, ya que como hemos visto en las pruebas de 50 conexiones, en cuanto empieza a aumentar la concurrencia de las solicitudes el rendimiento de la aplicación cae, llegando a un punto de las pruebas de 100 conexiones en el la aplicación comienza a estar sobrecargada.

\bigskip
Viendo el volumen de visitas actual, es difícil que a corto plazo se llegarán a producir este tipo de problemas de saturación, sin embargo, en futuro se deberían aplicar técnicas de balanceo de carga sobre la aplicación usando algún otro módulo de {\tt Node.js} como puede ser {\tt node-http-proxy}, cambiar la estructura de la aplicación para que esta no tenga problemas a unos niveles altos de carga de trabajo, o incluso cambiar el sistema operativo por otro que pudiera dar un mayor rendimiento en entornos de servidores.

\newpage
\section{Acceso web}

El resultado final es que el portal de transparencia sigue siendo accesible, y aunque visualmente no se aprecia ningún cambio, por debajo su funcionamiento y su desarrollo han cambiado completamente a como se ha descrito a lo largo de todo el documento.

\begin{figure}[!ht]
	\begin{center}
		\includegraphics[width=1\textwidth]{../images/transparente.png}
		\caption{Visualización del portal de transparencia}
		\label{fig:transparente}
	\end{center}
\end{figure}